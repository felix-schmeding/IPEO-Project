{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS is a jupyter notebook named inference.ipynb that \\\n",
    "a. loads at least one image/sample from the test set \\ \n",
    "b. loads trained parameters from the best model you trained \\\n",
    "c. runs inference (i.e. applies the model) on one image from the test set \\\n",
    "d. displays the predicJons for this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.CanopyDataset import CanopyDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a63a342fe954e609dba36fb7f0515c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='train_idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# * To get an overview of training set and to visualize all bands + label\n",
    "\n",
    "# plot individual samples from train set\n",
    "val_df = CanopyDataset(split='validation')\n",
    "#train_df = CanopyDataset(split='train', transforms=None)\n",
    "\n",
    "from ipywidgets import interact\n",
    "@interact(train_idx=range(len(val_df)))\n",
    "def plot_sample(train_idx=0):\n",
    "    train_img, train_label = val_df[train_idx]\n",
    "\n",
    "    # this is only to plot\n",
    "    if torch.is_tensor(train_img):\n",
    "        train_img = train_img.numpy()\n",
    "        train_img = np.transpose(train_img, (1, 2, 0))\n",
    "    print(train_img.shape)\n",
    "\n",
    "    f, axs = plt.subplots(2,6, figsize=(14,4), constrained_layout=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i in range(12):\n",
    "        sel = np.zeros(12, dtype=bool)\n",
    "        sel[i] = True\n",
    "        img = axs[i].imshow(train_img.compress(sel, axis=2))\n",
    "        axs[i].set_title(f\"Band {i}, index {train_idx}\")\n",
    "        # img = ax[0].imshow(train_img.cpu().detach())      # conversion to be able to plot\n",
    "        plt.colorbar(img)\n",
    "\n",
    "    f, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "    img = ax.imshow(train_label)\n",
    "    plt.colorbar(img)\n",
    "    ax.set_title(\"Label image\")\n",
    "\n",
    "    # f, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "    # sel = np.zeros(12, dtype=bool)\n",
    "    # sel[1:4] = True\n",
    "    # #print(train_img.compress(sel, axis=2))\n",
    "    # img = ax.imshow(train_img.compress(sel, axis=2))\n",
    "    # plt.colorbar(img)\n",
    "    # ax.set_title(\"Test RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataset = train_df = CanopyDataset(split='train')\n",
    "\n",
    "# # TODO create a training data dataloader with the specifications above\n",
    "# train_dl = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=1)    \t# or num_workers = 2?\n",
    "# for image, label in train_dl:\n",
    "#   print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# * to be implemented in a seperate file\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "\n",
    "def setup_optimiser(model, learning_rate, weight_decay):\n",
    "  return SGD(\n",
    "    model.parameters(),\n",
    "    learning_rate,\n",
    "    weight_decay\n",
    "  )\n",
    "\n",
    "from tqdm.notebook import trange      # pretty progress bar\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()   # ! need to change\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_epoch(data_loader, model, optimiser, device):\n",
    "\n",
    "  # set model to training mode. This is important because some layers behave differently during training and testing\n",
    "  model.train(True)\n",
    "  model.to(device)\n",
    "\n",
    "  # stats\n",
    "  loss_total = 0.0\n",
    "  oa_total = 0.0\n",
    "\n",
    "  # iterate over dataset\n",
    "  pBar = trange(len(data_loader))\n",
    "  for idx, (data, target) in enumerate(data_loader):\n",
    "    # put data and target onto correct device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # ! change to dataloader or dataset\n",
    "    data = data.to(torch.float32)   # to match weights of model\n",
    "    target = target.to(torch.float32) # to match data of model\n",
    "\n",
    "    # reset gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    pred = model(data)\n",
    "\n",
    "    # loss\n",
    "    target = target.squeeze()  # to match shape of pred\n",
    "    loss = criterion(pred, target)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    # parameter update\n",
    "    optimiser.step()\n",
    "\n",
    "    # stats update\n",
    "    loss_total += loss.item()\n",
    "    # ! probably need to change\n",
    "    acc = torch.mean(torch.abs(torch.sub(pred, target))).item()\n",
    "    oa_total += acc\n",
    "\n",
    "    # format progress bar\n",
    "    pBar.set_description('Loss: {:.2f}, OA: {:.2f}'.format(\n",
    "      loss_total/(idx+1),\n",
    "      oa_total/(idx+1)\n",
    "    ))\n",
    "    pBar.update(1)\n",
    "  \n",
    "  pBar.close()\n",
    "\n",
    "  # normalise stats\n",
    "  loss_total /= len(data_loader)\n",
    "  oa_total /= len(data_loader)\n",
    "\n",
    "  return model, loss_total, oa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(data_loader, model, device):       # note: no optimiser needed\n",
    "\n",
    "  # set model to evaluation mode\n",
    "  model.train(False)\n",
    "  model.to(device)\n",
    "\n",
    "  # stats\n",
    "  loss_total = 0.0\n",
    "  oa_total = 0.0\n",
    "\n",
    "  # iterate over dataset\n",
    "  pBar = trange(len(data_loader))\n",
    "  for idx, (data, target) in enumerate(data_loader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "      #TODO: likewise, implement the validation routine. This is very similar, but not identical, to the training steps.\n",
    "\n",
    "      # put data and target onto correct device\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      # ! change to dataloader or dataset\n",
    "      data = data.to(torch.float32)   # to match weights of model\n",
    "      target = target.to(torch.float32) # to match data of model\n",
    "\n",
    "      # forward pass\n",
    "      pred = model(data)\n",
    "\n",
    "      # loss\n",
    "      target = target.squeeze()  # to match shape of pred\n",
    "      loss = criterion(pred, target)\n",
    "\n",
    "      # stats update\n",
    "      loss_total += loss.item()\n",
    "      acc = torch.mean(torch.abs(torch.sub(pred, target))).item()\n",
    "      oa_total += acc\n",
    "\n",
    "      # format progress bar\n",
    "      pBar.set_description('Loss: {:.2f}, OA: {:.2f}'.format(\n",
    "        loss_total/(idx+1),\n",
    "        oa_total/(idx+1)\n",
    "      ))\n",
    "      pBar.update(1)\n",
    "\n",
    "  pBar.close()\n",
    "\n",
    "  # normalise stats\n",
    "  loss_total /= len(data_loader)\n",
    "  oa_total /= len(data_loader)\n",
    "\n",
    "  return loss_total, oa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model from file and not from notebook implementation\n",
    "from src.models.SIDE_code_decode import SIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# we also create a function for the data loader here (see Section 2.6 in Exercise 6)\n",
    "def load_dataloader(batch_size, dataset, split='train'):\n",
    "  return DataLoader(\n",
    "      dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=(split=='train'),       # we shuffle the image order for the training dataset\n",
    "      num_workers=2                   # perform data loading with two CPU threads\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! ------------------ model saving/loading\n",
    "\n",
    "import glob\n",
    "import os\n",
    "#from src.models.SIDE_code_decode import SIDE\n",
    "\n",
    "os.makedirs('cnn_states/SIDE', exist_ok=True)\n",
    "\n",
    "def load_model(epoch='latest'):\n",
    "  model = SIDE()\n",
    "  modelStates = glob.glob('cnn_states/SIDE/*.pth')\n",
    "  if len(modelStates) and (epoch == 'latest' or epoch > 0):\n",
    "    modelStates = [int(m.replace('cnn_states/SIDE/','').replace('.pth', '')) for m in modelStates]\n",
    "    if epoch == 'latest':\n",
    "      epoch = max(modelStates)\n",
    "    stateDict = torch.load(open(f'cnn_states/SIDE/{epoch}.pth', 'rb'), map_location='cpu')  # selects wieghts from epoch\n",
    "    model.load_state_dict(stateDict)\n",
    "  else:\n",
    "    # fresh model\n",
    "    epoch = 0       # no loaded weights\n",
    "  return model, epoch\n",
    "\n",
    "\n",
    "def save_model(model, epoch):\n",
    "  torch.save(model.state_dict(), open(f'cnn_states/SIDE/{epoch}.pth', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "device = 'cuda'\n",
    "start_epoch = 0        # set to 0 to start from scratch again or to 'latest' to continue training from saved checkpoint\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# * create all the needed variables\n",
    "train_test_df = CanopyDataset(split='train')\n",
    "val_test_df = CanopyDataset(split='validation')\n",
    "\n",
    "# dataloader\n",
    "dl_train_test = load_dataloader(batch_size, train_test_df)\n",
    "dl_val_test = load_dataloader(batch_size, val_test_df)\n",
    "\n",
    "# model\n",
    "model_test = SIDE()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim_test = setup_optimiser(model_test, learning_rate, weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c69ec241f444b48486f8a005e46a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264fc355fa1d46438dbd31fa3da0665f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 1/10] Loss train: 1462.20, val: 14834.87; OA train: 13.58, val: 62.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15227ebb1c1f4a03a9eb53cb82332771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e62b290e6264d628899b2657e4206bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 2/10] Loss train: 1376.49, val: 13995.63; OA train: 11.55, val: 61.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93823d7e3732418580c1123e438af761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d769b053849f4071a71d220e7f2f9689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 3/10] Loss train: 1373.59, val: 13944.91; OA train: 11.44, val: 60.37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ded70900764385bdff4a0ad05a6243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e58efac6f124dc583729bbfd8143b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 4/10] Loss train: 1371.42, val: 14027.08; OA train: 11.37, val: 64.84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6abe713ba8b42b78746edf9e8c9e544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3df50b39e2140efa72ba1e39a22968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 5/10] Loss train: 1370.63, val: 13626.12; OA train: 11.33, val: 67.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcb1da8cc55421e992685abe2edfcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4cb58630754466981ef3468aa861e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 6/10] Loss train: 1369.89, val: 13802.18; OA train: 11.29, val: 68.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3d1de340964b8e9adc3e5cb6b8de46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b90c5c1df4811be72c8ca1192b911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 7/10] Loss train: 1369.06, val: 14056.69; OA train: 11.27, val: 68.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3bdb146151462f932f4e0b8d553973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5838ce583fb4654bad185eed641daef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 8/10] Loss train: 1368.77, val: 14046.99; OA train: 11.24, val: 70.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e24a63913f4e07aa14eef0f4357cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557a108dce9c41058e133d96c7c2a0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 9/10] Loss train: 1368.43, val: 14437.69; OA train: 11.22, val: 71.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a117900a4c4f5a8f53f8db43840ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c37f187a8c48d597a2157e7b45183f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep. 10/10] Loss train: 1366.96, val: 14882.13; OA train: 11.21, val: 72.47\n"
     ]
    }
   ],
   "source": [
    "model_plot = []\n",
    "# do epochs\n",
    "while start_epoch < num_epochs:\n",
    "\n",
    "  # training\n",
    "  model_plot, loss_train, oa_train = train_epoch(dl_train_test, model_test, optim_test, device)\n",
    "\n",
    "  # validation\n",
    "  loss_val, oa_val = validate_epoch(dl_val_test, model_plot, device)\n",
    "\n",
    "  # print stats\n",
    "  print('[Ep. {}/{}] Loss train: {:.2f}, val: {:.2f}; OA train: {:.2f}, val: {:.2f}'.format(\n",
    "      start_epoch+1, num_epochs,\n",
    "      loss_train, loss_val,\n",
    "      oa_train, oa_val\n",
    "  ))\n",
    "\n",
    "  # save model\n",
    "  start_epoch += 1\n",
    "  save_model(model_plot, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc10b55651946e5bf115917633867dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx_val', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_df = CanopyDataset(split='validation')\n",
    "#train_df = CanopyDataset(split='train', transforms=None)\n",
    "\n",
    "from ipywidgets import interact\n",
    "@interact(idx_val=range(len(val_df)))\n",
    "def plot_sample(idx_val=0):\n",
    "    train_img, train_label = val_df[idx_val]\n",
    "    train_img = train_img.to(torch.float32).to('cuda')\n",
    "    \n",
    "\n",
    "    #data = data.to(torch.float32)   # to match weights of model\n",
    "    #target = target.to(torch.float32) # to match data of model\n",
    "\n",
    "    model_plot.train(False)\n",
    "    # as model expects batch number\n",
    "    train_img = model_plot(train_img.unsqueeze(0))\n",
    "    #model(image_valid.unsqueeze(0))\n",
    "\n",
    "    f, ax = plt.subplots(1,2, figsize=(6,6))\n",
    "    ax = ax.flatten()\n",
    "    img = ax[0].imshow(train_img.cpu().detach())      # conversion to be able to plot\n",
    "    plt.colorbar(img)\n",
    "    ax[0].set_title(\"Train image\")\n",
    "\n",
    "    img = ax[1].imshow(train_label)\n",
    "    plt.colorbar(img)\n",
    "    ax[1].set_title(\"Train label\")\n",
    "    plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPEO",
   "language": "python",
   "name": "ipeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
