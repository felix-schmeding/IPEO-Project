{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS is a jupyter notebook named inference.ipynb that \\\n",
    "a. loads at least one image/sample from the test set \\ \n",
    "b. loads trained parameters from the best model you trained \\\n",
    "c. runs inference (i.e. applies the model) on one image from the test set \\\n",
    "d. displays the predicJons for this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.CanopyDataset import CanopyDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b37a7133c554ab9aa7e1f5850250cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='train_idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# * To get an overview of training set and to visualize all bands + label\n",
    "\n",
    "# plot individual samples from train set\n",
    "val_df = CanopyDataset(split='validation')\n",
    "#train_df = CanopyDataset(split='train', transforms=None)\n",
    "\n",
    "from ipywidgets import interact\n",
    "@interact(train_idx=range(len(val_df)))\n",
    "def plot_sample(train_idx=0):\n",
    "    train_img, train_label = val_df[train_idx]\n",
    "\n",
    "    # this is only to plot\n",
    "    if torch.is_tensor(train_img):\n",
    "        train_img = train_img.numpy()\n",
    "        train_img = np.transpose(train_img, (1, 2, 0))\n",
    "    print(train_img.shape)\n",
    "\n",
    "    f, axs = plt.subplots(2,6, figsize=(14,4), constrained_layout=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i in range(12):\n",
    "        sel = np.zeros(12, dtype=bool)\n",
    "        sel[i] = True\n",
    "        img = axs[i].imshow(train_img.compress(sel, axis=2))\n",
    "        axs[i].set_title(f\"Band {i}, index {train_idx}\")\n",
    "        # img = ax[0].imshow(train_img.cpu().detach())      # conversion to be able to plot\n",
    "        plt.colorbar(img)\n",
    "\n",
    "    f, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "    img = ax.imshow(train_label)\n",
    "    plt.colorbar(img)\n",
    "    ax.set_title(\"Label image\")\n",
    "\n",
    "    # f, ax = plt.subplots(1,1, figsize=(3,3))\n",
    "    # sel = np.zeros(12, dtype=bool)\n",
    "    # sel[1:4] = True\n",
    "    # #print(train_img.compress(sel, axis=2))\n",
    "    # img = ax.imshow(train_img.compress(sel, axis=2))\n",
    "    # plt.colorbar(img)\n",
    "    # ax.set_title(\"Test RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataset = train_df = CanopyDataset(split='train')\n",
    "\n",
    "# # TODO create a training data dataloader with the specifications above\n",
    "# train_dl = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=1)    \t# or num_workers = 2?\n",
    "# for image, label in train_dl:\n",
    "#   print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# * to be implemented in a seperate file\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "\n",
    "def setup_optimiser(model, learning_rate, weight_decay):\n",
    "  return SGD(\n",
    "    model.parameters(),\n",
    "    learning_rate,\n",
    "    weight_decay\n",
    "  )\n",
    "\n",
    "from tqdm.notebook import trange      # pretty progress bar\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()   # ! need to change\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_epoch(data_loader, model, optimiser, device):\n",
    "\n",
    "  # set model to training mode. This is important because some layers behave differently during training and testing\n",
    "  model.train(True)\n",
    "  model.to(device)\n",
    "\n",
    "  # stats\n",
    "  loss_total = 0.0\n",
    "  oa_total = 0.0\n",
    "\n",
    "  # iterate over dataset\n",
    "  pBar = trange(len(data_loader))\n",
    "  for idx, (data, target) in enumerate(data_loader):\n",
    "    # put data and target onto correct device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # ! change to dataloader or dataset\n",
    "    data = data.to(torch.float32)   # to match weights of model\n",
    "    target = target.to(torch.float32) # to match data of model\n",
    "\n",
    "    # reset gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    pred = model(data)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(pred, target)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # parameter update\n",
    "    optimiser.step()\n",
    "\n",
    "    # stats update\n",
    "    loss_total += loss.item()\n",
    "    # ! probably need to change\n",
    "    acc = torch.mean(torch.abs(torch.sub(pred, target))).item()\n",
    "    oa_total += acc\n",
    "\n",
    "    # format progress bar\n",
    "    pBar.set_description('Loss: {:.2f}, OA: {:.2f}'.format(\n",
    "      loss_total/(idx+1),\n",
    "      100 * oa_total/(idx+1)\n",
    "    ))\n",
    "    pBar.update(1)\n",
    "  \n",
    "  pBar.close()\n",
    "\n",
    "  # normalise stats\n",
    "  loss_total /= len(data_loader)\n",
    "  oa_total /= len(data_loader)\n",
    "\n",
    "  return model, loss_total, oa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(data_loader, model, device):       # note: no optimiser needed\n",
    "\n",
    "  # set model to evaluation mode\n",
    "  model.train(False)\n",
    "  model.to(device)\n",
    "\n",
    "  # stats\n",
    "  loss_total = 0.0\n",
    "  oa_total = 0.0\n",
    "\n",
    "  # iterate over dataset\n",
    "  pBar = trange(len(data_loader))\n",
    "  for idx, (data, target) in enumerate(data_loader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "      #TODO: likewise, implement the validation routine. This is very similar, but not identical, to the training steps.\n",
    "\n",
    "      # put data and target onto correct device\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      # ! change to dataloader or dataset\n",
    "      data = data.to(torch.float32)   # to match weights of model\n",
    "      target = target.to(torch.float32) # to match data of model\n",
    "\n",
    "      # forward pass\n",
    "      pred = model(data)\n",
    "\n",
    "      # loss\n",
    "      loss = criterion(pred, target)\n",
    "\n",
    "      # stats update\n",
    "      loss_total += loss.item()\n",
    "      acc = torch.mean(torch.abs(torch.sub(pred, target))).item()\n",
    "      oa_total += acc\n",
    "\n",
    "      # format progress bar\n",
    "      pBar.set_description('Loss: {:.2f}, OA: {:.2f}'.format(\n",
    "        loss_total/(idx+1),\n",
    "        100 * oa_total/(idx+1)\n",
    "      ))\n",
    "      pBar.update(1)\n",
    "\n",
    "  pBar.close()\n",
    "\n",
    "  # normalise stats\n",
    "  loss_total /= len(data_loader)\n",
    "  oa_total /= len(data_loader)\n",
    "\n",
    "  return loss_total, oa_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! --------------------------\n",
    "# model to test, copy paste back when working\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def residual(in_chan, out_channel):\n",
    "    residual = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=in_chan),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_chan, out_channel, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "    return residual\n",
    "\n",
    "# def residual_maxpool(in_chan, out_channel):\n",
    "#     res_max = nn.Sequential(\n",
    "#         residual(in_chan, out_channel),\n",
    "#         nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "#     )\n",
    "\n",
    "#     return res_max\n",
    "\n",
    "def residual_decode(in_chan, out_channel):\n",
    "    residual = nn.Sequential(\n",
    "            # nn.MaxUnpool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(num_features=in_chan),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_chan, out_channel, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "    return residual\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, resblock):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # first block changes channel size, then keeps the same for the 2 other\n",
    "        self.sub1 = resblock(in_channel, out_channel)\n",
    "        self.sub23 = resblock(out_channel, out_channel)\n",
    "        \n",
    "        if in_channel < out_channel:\n",
    "            self.skip = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1)\n",
    "        elif in_channel > out_channel:\n",
    "            self.skip = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=1, stride=1)\n",
    "        else:\n",
    "            raise ValueError(\"Basis block: in_channel and out_channel should not be equal\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sub1(x)  # first block changes channel size\n",
    "        out = self.sub23(out)\n",
    "        out = self.sub23(out)\n",
    "\n",
    "        out = torch.add(out, self.skip(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SIDE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()  # super(self, SIDE).__init__() for backward compatiility\n",
    "\n",
    "        self.residualAdapt = BasicBlock(12, 64, residual)\n",
    "        # seperate as we need the result for the forward pass\n",
    "        \n",
    "        self.residual1 = BasicBlock(64, 128, residual)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n",
    "        # ! skip other ones, decode from 8 by 8\n",
    "\n",
    "\n",
    "        # * Upsampling\n",
    "\n",
    "        # unpool needs additionnal arg (indices), so seperate from residual block\n",
    "        # nn.Sequential doesn't allow for additional params\n",
    "        # https://stackoverflow.com/questions/59912850/autoencoder-maxunpool2d-missing-indices-argument\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "        self.up1 = BasicBlock(128, 64, residual_decode)\n",
    "        # maxunpool, \n",
    "        #self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=1)\n",
    "        # need element wise sum in forward before last residual block\n",
    "        self.final = BasicBlock(64, 1, residual_decode)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residualAdapt(x)     # get from 12 to 64 channels\n",
    "        #print(\"shape after resadapt : \" + str(x.shape))\n",
    "        out, ind1 = self.maxpool(x)    # first maxpool to 16x16x128\n",
    "        #print(\"shape after first_maxpool : \" + str(out.shape))\n",
    "        out = self.residual1(out)     \n",
    "        #print(\"shape after res1 : \" + str(out.shape))   \n",
    "        out, ind2 = self.maxpool(out)    # second maxpool to 8x8x256\n",
    "        print(\"shape after maxpool 2 : \" + str(out.shape)) \n",
    "        out = self.unpool(out, ind2)    # ind 2 as they are the last ones\n",
    "        #print(\"shape after unpool 1 : \" + str(out.shape))\n",
    "        out = self.up1(out)\n",
    "        #print(\"shape after up1 : \" + str(out.shape))\n",
    "        out = self.unpool(out, ind1)\n",
    "        #print(\"shape after unpool2 : \" + str(out.shape))\n",
    "\n",
    "        out = torch.add(x, out)\n",
    "        #print(\"shape out after add : \" + str(out.shape))\n",
    "        out = self.final(out)\n",
    "        print(\"shape out before squeeze : \" + str(out.shape))\n",
    "        # need to get 32x32 tensor to compare to label\n",
    "        out = out.squeeze()\n",
    "        print(\"shape out after squeeze : \" + str(out.shape))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# we also create a function for the data loader here (see Section 2.6 in Exercise 6)\n",
    "def load_dataloader(batch_size, dataset, split='train'):\n",
    "  return DataLoader(\n",
    "      dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=(split=='train'),       # we shuffle the image order for the training dataset\n",
    "      num_workers=2                   # perform data loading with two CPU threads\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ! ------------------ model saving/loading\n",
    "\n",
    "import glob\n",
    "import os\n",
    "#from src.models.SIDE_code_decode import SIDE\n",
    "\n",
    "os.makedirs('cnn_states/SIDE', exist_ok=True)\n",
    "\n",
    "def load_model(epoch='latest'):\n",
    "  model = SIDE()\n",
    "  modelStates = glob.glob('cnn_states/SIDE/*.pth')\n",
    "  if len(modelStates) and (epoch == 'latest' or epoch > 0):\n",
    "    modelStates = [int(m.replace('cnn_states/SIDE/','').replace('.pth', '')) for m in modelStates]\n",
    "    if epoch == 'latest':\n",
    "      epoch = max(modelStates)\n",
    "    stateDict = torch.load(open(f'cnn_states/SIDE/{epoch}.pth', 'rb'), map_location='cpu')  # selects wieghts from epoch\n",
    "    model.load_state_dict(stateDict)\n",
    "  else:\n",
    "    # fresh model\n",
    "    epoch = 0       # no loaded weights\n",
    "  return model, epoch\n",
    "\n",
    "\n",
    "def save_model(model, epoch):\n",
    "  torch.save(model.state_dict(), open(f'cnn_states/SIDE/{epoch}.pth', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.models.SIDE_code_decode import SIDE\n",
    "\n",
    "# define hyperparameters\n",
    "device = 'cuda'\n",
    "start_epoch = 0        # set to 0 to start from scratch again or to 'latest' to continue training from saved checkpoint\n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "num_epochs = 1\n",
    "\n",
    "# * create all the needed variables\n",
    "train_test_df = CanopyDataset(split='train')\n",
    "val_test_df = CanopyDataset(split='validation')\n",
    "\n",
    "# dataloader\n",
    "dl_train_test = load_dataloader(batch_size, train_test_df)\n",
    "dl_val_test = load_dataloader(batch_size, val_test_df)\n",
    "\n",
    "# model\n",
    "model_test = SIDE()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optim_test = setup_optimiser(model_test, learning_rate, weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "torch.set_printoptions(linewidth=200)\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "# defaults\n",
    "torch.set_printoptions(profile='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 218.,  217.,  217.,  ...,  223.,  223.,  223.],\n",
      "          [ 200.,  195.,  195.,  ...,  201.,  201.,  201.],\n",
      "          [ 186.,  178.,  178.,  ...,  184.,  184.,  184.],\n",
      "          ...,\n",
      "          [ 173.,  169.,  169.,  ...,  176.,  176.,  176.],\n",
      "          [ 173.,  169.,  169.,  ...,  176.,  176.,  176.],\n",
      "          [ 173.,  169.,  169.,  ...,  176.,  176.,  176.]],\n",
      "\n",
      "         [[ 359.,  375.,  372.,  ...,  330.,  324.,  331.],\n",
      "          [ 373.,  377.,  349.,  ...,  333.,  326.,  338.],\n",
      "          [ 378.,  384.,  357.,  ...,  328.,  329.,  336.],\n",
      "          ...,\n",
      "          [ 359.,  368.,  372.,  ...,  302.,  312.,  309.],\n",
      "          [ 332.,  351.,  370.,  ...,  282.,  297.,  305.],\n",
      "          [ 307.,  328.,  350.,  ...,  275.,  300.,  318.]],\n",
      "\n",
      "         [[ 787.,  829.,  792.,  ...,  728.,  696.,  717.],\n",
      "          [ 750.,  780.,  760.,  ...,  737.,  721.,  736.],\n",
      "          [ 714.,  759.,  764.,  ...,  708.,  696.,  706.],\n",
      "          ...,\n",
      "          [ 749.,  752.,  742.,  ...,  674.,  680.,  711.],\n",
      "          [ 710.,  725.,  731.,  ...,  666.,  677.,  696.],\n",
      "          [ 670.,  693.,  710.,  ...,  646.,  670.,  691.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2518., 2537., 2537.,  ..., 2853., 2853., 2853.],\n",
      "          [2573., 2588., 2588.,  ..., 2705., 2705., 2705.],\n",
      "          [2615., 2626., 2626.,  ..., 2595., 2595., 2595.],\n",
      "          ...,\n",
      "          [2714., 2683., 2683.,  ..., 2749., 2749., 2749.],\n",
      "          [2714., 2683., 2683.,  ..., 2749., 2749., 2749.],\n",
      "          [2714., 2683., 2683.,  ..., 2749., 2749., 2749.]],\n",
      "\n",
      "         [[2328., 2351., 2310.,  ..., 2326., 2315., 2299.],\n",
      "          [2285., 2340., 2286.,  ..., 2298., 2312., 2329.],\n",
      "          [2254., 2332., 2268.,  ..., 2278., 2309., 2352.],\n",
      "          ...,\n",
      "          [2197., 2210., 2203.,  ..., 2060., 2108., 2172.],\n",
      "          [2187., 2205., 2201.,  ..., 2014., 2061., 2125.],\n",
      "          [2143., 2154., 2156.,  ..., 2015., 2064., 2130.]],\n",
      "\n",
      "         [[1326., 1346., 1321.,  ..., 1293., 1275., 1250.],\n",
      "          [1298., 1326., 1288.,  ..., 1272., 1273., 1273.],\n",
      "          [1278., 1311., 1264.,  ..., 1257., 1271., 1290.],\n",
      "          ...,\n",
      "          [1208., 1214., 1214.,  ..., 1096., 1128., 1170.],\n",
      "          [1187., 1198., 1203.,  ..., 1058., 1092., 1138.],\n",
      "          [1153., 1158., 1168.,  ..., 1057., 1092., 1140.]]]], device='cuda:0')\n",
      "shape after maxpool 2 : torch.Size([1, 128, 8, 8])\n",
      "shape out before squeeze : torch.Size([1, 1, 32, 32])\n",
      "shape out after squeeze : torch.Size([32, 32])\n",
      "tensor([[2.0424e+03, 2.9044e+03, 1.6805e+03,  ..., 2.6391e+03, 2.8559e+03,\n",
      "         2.4146e+03],\n",
      "        [3.0471e+03, 2.1579e+03, 2.8782e+03,  ..., 3.4490e+03, 2.4726e+03,\n",
      "         2.4240e+03],\n",
      "        [2.2402e+03, 2.1691e+03, 1.6794e+03,  ..., 1.5814e+03, 2.4081e+03,\n",
      "         2.7551e+03],\n",
      "        ...,\n",
      "        [2.9590e+03, 3.5363e+03, 3.0296e+03,  ..., 3.0317e+03, 3.2360e+03,\n",
      "         2.2651e+03],\n",
      "        [2.0150e+03, 2.5700e+00, 2.9414e+03,  ..., 3.1546e+03, 2.7353e+03,\n",
      "         2.2688e+03],\n",
      "        [2.6353e+03, 2.5435e+03, 3.2604e+03,  ..., 2.8827e+03, 2.4125e+03,\n",
      "         2.8474e+03]], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "<class 'torch.Tensor'>  :  torch.float32  :  torch.Size([])\n",
      "OA : 2569.8984375, Loss : 6820496.0\n",
      "tensor([[[[ 433.,  431.,  423.,  ...,  398.,  398.,  398.],\n",
      "          [ 431.,  429.,  421.,  ...,  399.,  399.,  399.],\n",
      "          [ 421.,  419.,  414.,  ...,  400.,  400.,  400.],\n",
      "          ...,\n",
      "          [ 382.,  382.,  384.,  ...,  401.,  401.,  401.],\n",
      "          [ 382.,  382.,  384.,  ...,  401.,  401.,  401.],\n",
      "          [ 382.,  382.,  384.,  ...,  401.,  401.,  401.]],\n",
      "\n",
      "         [[ 436.,  438.,  447.,  ...,  389.,  390.,  392.],\n",
      "          [ 423.,  423.,  438.,  ...,  382.,  378.,  383.],\n",
      "          [ 411.,  415.,  430.,  ...,  385.,  379.,  380.],\n",
      "          ...,\n",
      "          [ 395.,  393.,  394.,  ...,  416.,  421.,  424.],\n",
      "          [ 395.,  400.,  400.,  ...,  413.,  421.,  422.],\n",
      "          [ 392.,  405.,  401.,  ...,  405.,  413.,  408.]],\n",
      "\n",
      "         [[ 669.,  675.,  693.,  ...,  607.,  590.,  580.],\n",
      "          [ 649.,  652.,  678.,  ...,  583.,  545.,  539.],\n",
      "          [ 635.,  653.,  678.,  ...,  577.,  544.,  548.],\n",
      "          ...,\n",
      "          [ 598.,  587.,  595.,  ...,  602.,  597.,  595.],\n",
      "          [ 589.,  599.,  611.,  ...,  594.,  598.,  599.],\n",
      "          [ 600.,  620.,  620.,  ...,  583.,  600.,  607.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4031., 4017., 3953.,  ..., 3659., 3659., 3659.],\n",
      "          [4033., 4021., 3965.,  ..., 3646., 3646., 3646.],\n",
      "          [4044., 4039., 4016.,  ..., 3593., 3593., 3593.],\n",
      "          ...,\n",
      "          [3712., 3721., 3763.,  ..., 3571., 3571., 3571.],\n",
      "          [3712., 3721., 3763.,  ..., 3571., 3571., 3571.],\n",
      "          [3712., 3721., 3763.,  ..., 3571., 3571., 3571.]],\n",
      "\n",
      "         [[1772., 1802., 1838.,  ..., 1803., 1814., 1828.],\n",
      "          [1743., 1773., 1824.,  ..., 1791., 1784., 1795.],\n",
      "          [1711., 1747., 1810.,  ..., 1777., 1768., 1772.],\n",
      "          ...,\n",
      "          [1738., 1746., 1747.,  ..., 1859., 1855., 1850.],\n",
      "          [1749., 1773., 1790.,  ..., 1826., 1813., 1808.],\n",
      "          [1776., 1803., 1818.,  ..., 1806., 1802., 1798.]],\n",
      "\n",
      "         [[ 747.,  754.,  762.,  ...,  777.,  786.,  792.],\n",
      "          [ 735.,  741.,  752.,  ...,  774.,  778.,  783.],\n",
      "          [ 718.,  727.,  749.,  ...,  767.,  768.,  768.],\n",
      "          ...,\n",
      "          [ 745.,  749.,  738.,  ...,  812.,  812.,  810.],\n",
      "          [ 754.,  763.,  759.,  ...,  794.,  790.,  789.],\n",
      "          [ 758.,  767.,  766.,  ...,  780.,  778.,  780.]]]], device='cuda:0')\n",
      "shape after maxpool 2 : torch.Size([1, 128, 8, 8])\n",
      "shape out before squeeze : torch.Size([1, 1, 32, 32])\n",
      "shape out after squeeze : torch.Size([32, 32])\n",
      "tensor([[ 3.6867e+12,  3.6918e+12,  3.7174e+12,  ...,  2.3301e+18,\n",
      "          3.4465e+12,  3.4418e+12],\n",
      "        [ 3.6239e+12,  3.6345e+12, -3.1841e+18,  ..., -2.9118e+18,\n",
      "          3.3377e+12,  3.3209e+12],\n",
      "        [ 3.6655e+18,  3.6461e+12,  3.7436e+12,  ...,  3.3120e+12,\n",
      "          3.2628e+12,  3.2793e+12],\n",
      "        ...,\n",
      "        [ 3.3817e+12,  3.4099e+12, -2.5925e+18,  ..., -2.5926e+18,\n",
      "          3.2956e+12,  3.3137e+12],\n",
      "        [ 3.3884e+12,  3.4582e+12, -2.9978e+18,  ...,  3.3092e+12,\n",
      "         -2.8561e+18,  3.3565e+12],\n",
      "        [ 3.4606e+12,  3.5378e+12,  3.6203e+12,  ...,  3.3514e+12,\n",
      "          3.4254e+12,  2.2844e+18]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "<class 'torch.Tensor'>  :  torch.float32  :  torch.Size([])\n",
      "OA : 7.195005492878377e+17, Loss : inf\n",
      "tensor([[[[ 423.,  423.,  423.,  ...,  476.,  486.,  501.],\n",
      "          [ 432.,  432.,  432.,  ...,  487.,  491.,  498.],\n",
      "          [ 432.,  432.,  432.,  ...,  487.,  491.,  498.],\n",
      "          ...,\n",
      "          [ 446.,  446.,  446.,  ...,  474.,  473.,  471.],\n",
      "          [ 446.,  446.,  446.,  ...,  474.,  473.,  471.],\n",
      "          [ 446.,  446.,  446.,  ...,  474.,  473.,  471.]],\n",
      "\n",
      "         [[ 344.,  321.,  304.,  ...,  353.,  357.,  389.],\n",
      "          [ 378.,  355.,  327.,  ...,  381.,  393.,  419.],\n",
      "          [ 384.,  382.,  366.,  ...,  415.,  422.,  436.],\n",
      "          ...,\n",
      "          [ 388.,  387.,  372.,  ...,  386.,  385.,  391.],\n",
      "          [ 408.,  407.,  377.,  ...,  371.,  386.,  398.],\n",
      "          [ 411.,  416.,  384.,  ...,  360.,  370.,  379.]],\n",
      "\n",
      "         [[ 439.,  377.,  319.,  ...,  428.,  432.,  507.],\n",
      "          [ 513.,  471.,  402.,  ...,  478.,  520.,  588.],\n",
      "          [ 540.,  546.,  496.,  ...,  570.,  628.,  659.],\n",
      "          ...,\n",
      "          [ 527.,  519.,  480.,  ...,  513.,  545.,  580.],\n",
      "          [ 544.,  530.,  489.,  ...,  492.,  530.,  560.],\n",
      "          [ 542.,  549.,  513.,  ...,  474.,  493.,  513.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2707., 2707., 2707.,  ..., 3819., 4205., 4766.],\n",
      "          [3225., 3225., 3225.,  ..., 3772., 4285., 5028.],\n",
      "          [3225., 3225., 3225.,  ..., 3772., 4285., 5028.],\n",
      "          ...,\n",
      "          [3373., 3373., 3373.,  ..., 3307., 3353., 3420.],\n",
      "          [3373., 3373., 3373.,  ..., 3307., 3353., 3420.],\n",
      "          [3373., 3373., 3373.,  ..., 3307., 3353., 3420.]],\n",
      "\n",
      "         [[1385., 1248., 1097.,  ..., 1356., 1519., 1750.],\n",
      "          [1512., 1404., 1274.,  ..., 1481., 1698., 1981.],\n",
      "          [1627., 1586., 1521.,  ..., 1791., 1970., 2174.],\n",
      "          ...,\n",
      "          [1770., 1680., 1592.,  ..., 1533., 1546., 1572.],\n",
      "          [1705., 1668., 1628.,  ..., 1411., 1425., 1453.],\n",
      "          [1620., 1654., 1676.,  ..., 1250., 1266., 1297.]],\n",
      "\n",
      "         [[ 609.,  554.,  490.,  ...,  548.,  629.,  742.],\n",
      "          [ 668.,  629.,  576.,  ...,  599.,  707.,  850.],\n",
      "          [ 705.,  686.,  656.,  ...,  739.,  834.,  945.],\n",
      "          ...,\n",
      "          [ 740.,  712.,  678.,  ...,  630.,  643.,  662.],\n",
      "          [ 716.,  709.,  694.,  ...,  569.,  583.,  606.],\n",
      "          [ 686.,  704.,  713.,  ...,  490.,  505.,  533.]]]], device='cuda:0')\n",
      "shape after maxpool 2 : torch.Size([1, 128, 8, 8])\n",
      "shape out before squeeze : torch.Size([1, 1, 32, 32])\n",
      "shape out after squeeze : torch.Size([32, 32])\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "<class 'torch.Tensor'>  :  torch.float32  :  torch.Size([])\n",
      "OA : nan, Loss : nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programme\\miniconda3\\envs\\IPEO\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1, 32, 32])) that is different to the input size (torch.Size([32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totals:\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# only one step\n",
    "model = SIDE()\n",
    "data_loader = load_dataloader(batch_size, train_test_df)\n",
    "optimiser = setup_optimiser(model, learning_rate, weight_decay)\n",
    "\n",
    "model.train(True)\n",
    "model.to(device)\n",
    "\n",
    "# stats\n",
    "loss_total = 0.0\n",
    "oa_total = 0.0\n",
    "\n",
    "num_im = 1\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for idx, (data, target) in enumerate(data_loader):\n",
    "    # put data and target onto correct device\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # ! change to dataloader or dataset\n",
    "    data = data.to(torch.float32)   # to match weights of model\n",
    "    target = target.to(torch.float32) # to match data of model\n",
    "    print(data)\n",
    "    # reset gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    pred = model(data)\n",
    "    print(pred)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(pred, target)\n",
    "    print(str(type(loss)) + '  :  ' + str(loss.dtype)+ '  :  ' + str(loss.shape))\n",
    "    #print(loss)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # parameter update\n",
    "    optimiser.step()\n",
    "\n",
    "    # stats update\n",
    "    loss_total += loss.item()\n",
    "    # mean of absolute per pixel height differences from predicted height and GT\n",
    "    acc = torch.mean(torch.abs(torch.sub(pred, target))).item()\n",
    "    oa_total += acc\n",
    "    print('OA : ' + str(acc) + \", Loss : \" + str(loss_total))\n",
    "\n",
    "    #to do only 1 to test\n",
    "    if idx > num_im:    \n",
    "        break\n",
    "    \n",
    "\n",
    "# normalise stats\n",
    "loss_total /= num_im\n",
    "oa_total /= num_im\n",
    "print('totals:')\n",
    "print(loss_total)\n",
    "print(oa_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIDE(\n",
       "  (residualAdapt): BasicBlock(\n",
       "    (sub1): Sequential(\n",
       "      (0): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(12, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (sub23): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (skip): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (residual1): BasicBlock(\n",
       "    (sub1): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (sub23): Sequential(\n",
       "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "  (up1): BasicBlock(\n",
       "    (sub1): Sequential(\n",
       "      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (sub23): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (skip): ConvTranspose2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (final): BasicBlock(\n",
       "    (sub1): Sequential(\n",
       "      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (sub23): Sequential(\n",
       "      (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (skip): ConvTranspose2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just forward test\n",
    "\n",
    "moooodel = SIDE()\n",
    "daaaaaata_loader = load_dataloader(batch_size, train_test_df)\n",
    "moooodel.train(True)\n",
    "moooodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after resadapt : torch.Size([1, 64, 32, 32])\n",
      "shape after first_maxpool : torch.Size([1, 64, 16, 16])\n",
      "shape after res1 : torch.Size([1, 128, 16, 16])\n",
      "shape after maxpool 2 : torch.Size([1, 128, 8, 8])\n",
      "shape after unpool 1 : torch.Size([1, 128, 16, 16])\n",
      "shape after up1 : torch.Size([1, 64, 16, 16])\n",
      "shape after unpool2 : torch.Size([1, 64, 32, 32])\n",
      "shape out after add : torch.Size([1, 64, 32, 32])\n",
      "shape out before squeeze : torch.Size([1, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_features, train_labels = next(iter(daaaaaata_loader))\n",
    "train_features = train_features.to(torch.float32)   # to match weights of model\n",
    "train_features = train_features.to(device)   # gpu\n",
    "prediction = moooodel(train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(71358568., device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_labels = train_labels.to(torch.float32)   # to match weights of model\n",
    "train_labels = train_labels.to(device)   # gpu\n",
    "loss = criterion(prediction, train_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8084.7222, -7367.8823, -8722.4756,  ..., -8477.9854, -8405.3740,\n",
       "         -8058.3184],\n",
       "        [-8655.1709, -8201.1855, -8712.6309,  ..., -8743.8457, -8581.2998,\n",
       "         -7356.8394],\n",
       "        [-8717.6592, -7914.3745, -8016.9028,  ..., -8457.9268, -7737.3047,\n",
       "         -8572.5615],\n",
       "        ...,\n",
       "        [-8336.4814, -8465.9326, -8434.6738,  ..., -8103.1392, -8272.9785,\n",
       "         -8124.2061],\n",
       "        [-7503.4219, -7709.6348, -8821.5059,  ..., -8255.4219, -8244.3174,\n",
       "         -8138.9893],\n",
       "        [-9022.5742, -7567.5410, -8480.3330,  ..., -7324.6265, -7779.1787,\n",
       "         -7610.1377]], device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do epochs\n",
    "while start_epoch < num_epochs:\n",
    "\n",
    "  # training\n",
    "  model, loss_train, oa_train = train_epoch(dl_train_test, model_test, optim_test, device)\n",
    "\n",
    "  # validation\n",
    "  loss_val, oa_val = validate_epoch(dl_val_test, model, device)\n",
    "\n",
    "  # print stats\n",
    "  print('[Ep. {}/{}] Loss train: {:.2f}, val: {:.2f}; OA train: {:.2f}, val: {:.2f}'.format(\n",
    "      start_epoch+1, num_epochs,\n",
    "      loss_train, loss_val,\n",
    "      100*oa_train, 100*oa_val\n",
    "  ))\n",
    "\n",
    "  # save model\n",
    "  start_epoch += 1\n",
    "  save_model(model, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe65a1ee97e04519a7693df6c9f9dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx_val', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_plot = model\n",
    "\n",
    "val_df = CanopyDataset(split='validation')\n",
    "#train_df = CanopyDataset(split='train', transforms=None)\n",
    "\n",
    "from ipywidgets import interact\n",
    "@interact(idx_val=range(len(val_df)))\n",
    "def plot_sample(idx_val=0):\n",
    "    train_img, train_label = val_df[idx_val]\n",
    "    train_img = train_img.to(torch.float32).to('cuda')\n",
    "    \n",
    "\n",
    "    #data = data.to(torch.float32)   # to match weights of model\n",
    "    #target = target.to(torch.float32) # to match data of model\n",
    "\n",
    "    model_plot.train(False)\n",
    "    # as model expects batch number\n",
    "    train_img = model_plot(train_img.unsqueeze(0))\n",
    "    #model(image_valid.unsqueeze(0))\n",
    "\n",
    "    f, ax = plt.subplots(1,2, figsize=(6,6))\n",
    "    ax = ax.flatten()\n",
    "    img = ax[0].imshow(train_img.cpu().detach())      # conversion to be able to plot\n",
    "    plt.colorbar(img)\n",
    "    ax[0].set_title(\"Train image\")\n",
    "\n",
    "    img = ax[1].imshow(train_label)\n",
    "    plt.colorbar(img)\n",
    "    ax[1].set_title(\"Train label\")\n",
    "    plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPEO",
   "language": "python",
   "name": "ipeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
